{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4WsRJ2pTeKp"
      },
      "source": [
        "## Проверка шаблона"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEcJjmg8TjOO"
      },
      "source": [
        "Перед классификацией патента (т.е. к какому классу патентов он относится), необходимо провести проверку, что он удовлетворяет требованиям оформления. В качестве шаблона был взят шаблон из примеров файлов по следующей ссылке: https://neustel.com/patents/sample-patents/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNhslbPZ7tq_"
      },
      "outputs": [],
      "source": [
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGz_jK9STVG9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from io import StringIO\n",
        "import pdfminer as miner\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.high_level import extract_text_to_fp\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "\n",
        "    output_string = StringIO()\n",
        "\n",
        "    with open(file_path, 'rb') as file:\n",
        "        extract_text_to_fp(file, output_string, laparams=LAParams(), page_numbers=[0, 1])\n",
        "\n",
        "    return output_string.getvalue()\n",
        "\n",
        "def process_text(text):\n",
        "\n",
        "    patterns = [\n",
        "    (r\"(.*?)(\\(57\\).*)\", r\"\\1\", \"sub\"),\n",
        "    (r\"(\\(12\\).*)\", None, \"search\"),\n",
        "    (r\"(\\(51\\).*?)(\\(74\\))\", r\"\\2\", \"sub\"),\n",
        "    (r\"(\\(54\\).*?)(\\(76\\))\", r\"\\2\", \"sub\"),\n",
        "    (r\"\\(\\s*\\*\\s*\\).*?\\(22\\)\", \"\", \"sub\"),\n",
        "    # (r\"Filed.*?\\(74\\)\", \"\", \"sub\"),\n",
        "    # (r\"Filed.*?\\(60\\)\", \"\", \"sub\"),\n",
        "    # (r\"Filed.*\", \"\", \"sub\")\n",
        "    ]\n",
        "\n",
        "    for pattern, replacement, action in patterns:\n",
        "        if action == \"sub\":\n",
        "            text = re.sub(pattern, replacement, text, flags=re.DOTALL)\n",
        "        elif action == \"search\":\n",
        "            match = re.search(pattern, text, re.DOTALL)\n",
        "            if match:\n",
        "                text = match.group(1)\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_all_sections(text):\n",
        "    pattern = r\"\\((\\d{1,3})\\)\"\n",
        "\n",
        "    sections = [match for match in re.finditer(pattern, text)]\n",
        "\n",
        "    content_between_sections = []\n",
        "\n",
        "    for i in range(len(sections)-1):\n",
        "        start_section = int(sections[i].group(1))\n",
        "        end_section = int(sections[i+1].group(1))\n",
        "\n",
        "        if 1 <= start_section <= 200 and 1 <= end_section <= 200:\n",
        "            start_index = sections[i].end()\n",
        "            end_index = sections[i+1].start()\n",
        "            content = text[start_index:end_index].strip()\n",
        "\n",
        "            content_between_sections.append(f'({start_section})' + content)\n",
        "\n",
        "    content_between_sections.append(text[sections[len(sections)-1].start():])\n",
        "    return content_between_sections\n",
        "\n",
        "def process_text_items(text_items):\n",
        "    processed_items = []\n",
        "    patent_no = \"\"\n",
        "\n",
        "    for item in text_items:\n",
        "        if \"Bl\" in item and any(char.isdigit() for char in item):\n",
        "            item_copy = item\n",
        "            item_lst = item_copy.split('\\n')\n",
        "            for i in range(len(item_lst)):\n",
        "              if \"Bl\" in item_lst[i]:\n",
        "                patent_no = item_lst[i]\n",
        "                break\n",
        "\n",
        "        if item.startswith('(45)Date of Patent:'):\n",
        "            date_match = re.search(r\"(\\b\\w{3}\\.?\\s+\\d{1,2},\\s+\\d{4}\\b)\", item)\n",
        "            if date_match:\n",
        "                date = date_match.group(0)\n",
        "                item = f'(45)Date of Patent: {date}'\n",
        "\n",
        "        processed_items.append(item)\n",
        "\n",
        "    for i in range(len(processed_items)):\n",
        "      if '(10)Patent No.:' in processed_items[i]:\n",
        "        processed_items[i] = f'(10)Patent No.: {patent_no}'\n",
        "\n",
        "    return processed_items\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/PatentClassification/Sample-Patent_Railroad_Flangeway_Cleaner_System.pdf\"\n",
        "text = extract_text_from_pdf(file_path)\n",
        "processed_text = process_text(text)\n",
        "list_text = extract_all_sections(processed_text)\n",
        "processed_list = process_text_items(list_text)\n",
        "\n",
        "for item in processed_list:\n",
        "    print(item)\n",
        "\n",
        "match_12 = any(\"(12)United States Patent\" in item for item in processed_list)\n",
        "match_10 = any(\"(10)Patent No.: US\" in item for item in processed_list)\n",
        "match_45 = any(\"(45)Date of Patent:\" in item for item in processed_list)\n",
        "match_76 = any(\"(76)Inventor:\" in item for item in processed_list)\n",
        "\n",
        "if match_12 and match_10 and match_45 and match_76:\n",
        "    print(\"The document matches the template\")\n",
        "else:\n",
        "    print(\"The document does not match the template\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgXoq8rU9FNr"
      },
      "source": [
        "## Выделение содержания"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWAwVmn09KnE"
      },
      "outputs": [],
      "source": [
        "from pdfminer.high_level import extract_text_to_fp\n",
        "from pdfminer.layout import LAParams\n",
        "from io import StringIO\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    output_string = StringIO()\n",
        "\n",
        "    with open(file_path, 'rb') as file:\n",
        "        extract_text_to_fp(file, output_string, laparams=LAParams())\n",
        "\n",
        "    extracted_text = output_string.getvalue()\n",
        "\n",
        "    background_index = extracted_text.find(\"BACKGROUND OF THE INVENTION\")\n",
        "\n",
        "    if background_index != -1:\n",
        "        text_after_background = extracted_text[background_index + len(\"BACKGROUND OF THE INVENTION\"):].strip()\n",
        "        return text_after_background.replace('\\n\\n', ' ')\n",
        "    else:\n",
        "        return \"Background section not found in the document.\"\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/PatentClassification/Sample-Patent_Railroad_Flangeway_Cleaner_System.pdf\"\n",
        "text_after_background = extract_text_from_pdf(file_path)\n",
        "print(text_after_background)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOJZ1lVcPq3i"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Нами будет решаться задача многоклассовой классификации патентов. Поэтому первым шагом является предобработка набора данных. Используемый датасет был выбра с HaggingFace по ссылке: https://huggingface.co/datasets/ccdv/patent-classification\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvt5q6vTbmYe"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUmhKObx0k-W"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KrzsvSkQ56p"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"/content/drive/MyDrive/PatentClassification/patent_classification_dataset.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkKHzNIfDMbJ"
      },
      "source": [
        "Токенизация данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSij4Sv9Dg7p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AdamW\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gRrltEgDKae"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
        "\n",
        "_LABELS = [\n",
        "    \"Human Necessities\",\n",
        "    \"Performing Operations; Transporting\",\n",
        "    \"Chemistry; Metallurgy\",\n",
        "    \"Textiles; Paper\",\n",
        "    \"Fixed Constructions\",\n",
        "    \"Mechanical Engineering; Lightning; Heating; Weapons; Blasting\",\n",
        "    \"Physics\",\n",
        "    \"Electricity\",\n",
        "    \"General tagging of new or cross-sectional technology\",\n",
        "]\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"A custom dataset class for your patents data.\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text,\n",
        "                                  add_special_tokens=True,\n",
        "                                  max_length=self.max_len,\n",
        "                                  padding='max_length',\n",
        "                                  truncation=True,\n",
        "                                  return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "# 1\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#2\n",
        "#model_name = 'distilbert-base-uncased'\n",
        "#tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#3\n",
        "# model_name = 'google/electra-base-discriminator'\n",
        "# tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#4\n",
        "# model_name = 'xlnet-base-cased'\n",
        "# tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
        "\n",
        "train_texts, train_labels = dataset['train']['text'][:15000], dataset['train']['label'][:15000]\n",
        "val_texts, val_labels = dataset['validation']['text'], dataset['validation']['label']\n",
        "test_texts, test_labels = dataset['test']['text'], dataset['test']['label']\n",
        "\n",
        "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
        "test_dataset = CustomDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_vAgt2vT-Pg"
      },
      "source": [
        "## Training and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsVI0hUGUGwc"
      },
      "source": [
        "Приступаем к загрузке и обучению/дообучению модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNwngsb2D0nz"
      },
      "source": [
        "Загружаем предобученную модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRipVQgUD0FS"
      },
      "outputs": [],
      "source": [
        "num_labels = 9\n",
        "\n",
        "#1\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "#2\n",
        "#model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "\n",
        "#3\n",
        "# model = ElectraForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "#4\n",
        "# model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=0.001)#2e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShC9wgR-D8QU"
      },
      "source": [
        "Дообучение модели на новых данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSX10VdleQbZ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, n_examples, epoch, epochs=3):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    progress_bar = tqdm(data_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        loss = torch.nn.CrossEntropyLoss()(outputs.logits, labels)\n",
        "        # loss_func = torch.nn.Softmax(dim=1)\n",
        "        # loss = loss_func(outputs.logits)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        progress_bar.set_postfix({'Loss': loss.item(), 'Accuracy': correct_predictions.item() / n_examples})\n",
        "\n",
        "    return correct_predictions.item() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples, epoch, epochs=3):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(data_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            loss = torch.nn.CrossEntropyLoss()(outputs.logits, labels)\n",
        "            # loss_func = torch.nn.Softmax(dim=1)\n",
        "            # loss = loss_func(outputs.logits)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            progress_bar.set_postfix({'Loss': loss.item(), 'Accuracy': correct_predictions.item() / n_examples})\n",
        "\n",
        "    return correct_predictions.item() / n_examples, np.mean(losses)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "train_losses = []\n",
        "train_acces = []\n",
        "val_losses = []\n",
        "val_acces = []\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(model, train_loader, None, optimizer, device, len(train_dataset), epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    train_acces.append(train_acc)\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss = eval_model(model, val_loader, None, device, len(val_dataset), epoch)\n",
        "    val_losses.append(val_loss)\n",
        "    val_acces.append(val_acc)\n",
        "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "    print()\n",
        "\n",
        "    finish_time = time.time()\n",
        "    print(\"Total time for 1 Epoch: \", (finish_time - start_time) / 60)\n",
        "    print()\n",
        "\n",
        "# Save the trained model\n",
        "if '/' in model_name:\n",
        "    update_name = model_name.split('/')[1]\n",
        "else:\n",
        "    update_name = model_name\n",
        "\n",
        "torch.save(model.state_dict(), update_name + \"_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYH4hKFeS2r3"
      },
      "source": [
        "Тестирование"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPhEJB8dwcBV"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "predictions, true_labels = get_predictions(model, test_loader, device)\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "report = classification_report(true_labels, predictions, target_names=_LABELS)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Classification Report:\\n{report}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0GcoCi__atK"
      },
      "source": [
        "## Тестирование для конкретного примера"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5NligcMH_eZ1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
        "from datasets import Dataset\n",
        "\n",
        "from pdfminer.high_level import extract_text_to_fp\n",
        "from pdfminer.layout import LAParams\n",
        "from io import StringIO\n",
        "import torch\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    output_string = StringIO()\n",
        "\n",
        "    with open(file_path, 'rb') as file:\n",
        "        extract_text_to_fp(file, output_string, laparams=LAParams())\n",
        "\n",
        "    extracted_text = output_string.getvalue()\n",
        "\n",
        "    background_index = extracted_text.find(\"BACKGROUND OF THE INVENTION\")\n",
        "\n",
        "    if background_index != -1:\n",
        "        text_after_background = extracted_text[background_index + len(\"BACKGROUND OF THE INVENTION\"):].strip()\n",
        "        return text_after_background.replace('\\n\\n', ' ')\n",
        "    else:\n",
        "        return \"Background section not found in the document.\"\n",
        "\n",
        "def classify_text(model, tokenizer, text, device):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    probabilities = torch.softmax(logits, dim=1).squeeze().tolist()\n",
        "    return probabilities\n",
        "\n",
        "model_name = 'xlnet-base-cased' #CHANGE NAME\n",
        "tokenizer = XLNetTokenizer.from_pretrained(model_name) #CHANGE NAME\n",
        "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=num_labels) #CHANGE NAME\n",
        "\n",
        "model.load_state_dict(torch.load(update_name + \"_model.pt\")) # внутри torch.load нужно указать имя модели\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "uploaded_file_path = \"/content/drive/MyDrive/PatentClassification/Sample-Patent_Railroad_Flangeway_Cleaner_System.pdf\"  # Update with the path to your uploaded file\n",
        "text_to_classify = extract_text_from_pdf(uploaded_file_path)\n",
        "\n",
        "probabilities = classify_text(model, tokenizer, text_to_classify, device)\n",
        "\n",
        "_LABELS = [\n",
        "    \"Human Necessities\",\n",
        "    \"Performing Operations; Transporting\",\n",
        "    \"Chemistry; Metallurgy\",\n",
        "    \"Textiles; Paper\",\n",
        "    \"Fixed Constructions\",\n",
        "    \"Mechanical Engineering; Lightning; Heating; Weapons; Blasting\",\n",
        "    \"Physics\",\n",
        "    \"Electricity\",\n",
        "    \"General tagging of new or cross-sectional technology\",\n",
        "]\n",
        "for label, prob in zip(_LABELS, probabilities):\n",
        "    print(f'{label}: {prob * 100} %')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV4tFWt-KkRP"
      },
      "source": [
        "## Визуализация результатов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TzhPImyKmhU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = [1, 2, 3]  # Example epochs, but can be more\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses, label='Training Loss')\n",
        "plt.plot(epochs, val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_acces, label='Training Accuracy')\n",
        "plt.plot(epochs, val_acces, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-9_mOXvz78g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AdamW\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
        "\n",
        "dataset = load_dataset(\"/content/drive/MyDrive/PatentClassification/patent_classification_dataset.py\")\n",
        "\n",
        "_LABELS = [\n",
        "    \"Human Necessities\",\n",
        "    \"Performing Operations; Transporting\",\n",
        "    \"Chemistry; Metallurgy\",\n",
        "    \"Textiles; Paper\",\n",
        "    \"Fixed Constructions\",\n",
        "    \"Mechanical Engineering; Lightning; Heating; Weapons; Blasting\",\n",
        "    \"Physics\",\n",
        "    \"Electricity\",\n",
        "    \"General tagging of new or cross-sectional technology\",\n",
        "]\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"A custom dataset class for your patents data.\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text,\n",
        "                                  add_special_tokens=True,\n",
        "                                  max_length=self.max_len,\n",
        "                                  padding='max_length',\n",
        "                                  truncation=True,\n",
        "                                  return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "# 1\n",
        "#model_name = \"bert-base-uncased\"\n",
        "#tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#2\n",
        "#model_name = 'distilbert-base-uncased'\n",
        "#tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#3\n",
        "# model_name = 'google/electra-base-discriminator'\n",
        "# tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#4\n",
        "model_name = 'xlnet-base-cased'\n",
        "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
        "\n",
        "train_texts, train_labels = dataset['train']['text'][:15000], dataset['train']['label'][:15000]\n",
        "val_texts, val_labels = dataset['validation']['text'], dataset['validation']['label']\n",
        "test_texts, test_labels = dataset['test']['text'], dataset['test']['label']\n",
        "\n",
        "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
        "test_dataset = CustomDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "import time\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, n_examples, epoch, epochs=3):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    progress_bar = tqdm(data_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        loss = torch.nn.CrossEntropyLoss()(outputs.logits, labels)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        progress_bar.set_postfix({'Loss': loss.item(), 'Accuracy': correct_predictions.item() / n_examples})\n",
        "\n",
        "    return correct_predictions.item() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples, epoch, epochs=3):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(data_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            loss = torch.nn.CrossEntropyLoss()(outputs.logits, labels)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            progress_bar.set_postfix({'Loss': loss.item(), 'Accuracy': correct_predictions.item() / n_examples})\n",
        "\n",
        "    return correct_predictions.item() / n_examples, np.mean(losses)\n",
        "\n",
        "def get_predictions(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "train_losses = []\n",
        "train_acces = []\n",
        "val_losses = []\n",
        "val_acces = []\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(model, train_loader, None, optimizer, device, len(train_dataset), epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    train_acces.append(train_acc)\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss = eval_model(model, val_loader, None, device, len(val_dataset), epoch)\n",
        "    val_losses.append(val_loss)\n",
        "    val_acces.append(val_acc)\n",
        "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "    print()\n",
        "\n",
        "    finish_time = time.time()\n",
        "    print(\"Total time for 1 Epoch: \", (finish_time - start_time) / 60)\n",
        "    print()\n",
        "\n",
        "# Save the trained model\n",
        "if '/' in model_name:\n",
        "    update_name = model_name.split('/')[1]\n",
        "else:\n",
        "    update_name = model_name\n",
        "\n",
        "torch.save(model.state_dict(), update_name + \"_model.pt\")\n",
        "\n",
        "predictions, true_labels = get_predictions(model, test_loader, device)\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "report = classification_report(true_labels, predictions, target_names=_LABELS)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Classification Report:\\n{report}\")"
      ],
      "metadata": {
        "id": "l-AqkMnF7i63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "lst = [1, 2, 3]\n",
        "\n",
        "np.mean(lst)"
      ],
      "metadata": {
        "id": "V7sUW0JWRCoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lgFQUm3XRGzL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}